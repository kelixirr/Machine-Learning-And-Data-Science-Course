{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization & Text Processing In NLP By Neuraldemy\nThis tutorial is part of Neuraldemy's in depth tutorial on NLP. This notebook contains discussion regarding how we can deal with our text corpus and other preprocessing steps. This also includes the details on various tokenization schemes avaliable such as BPE etc. \n___\n**Author**: Amritesh Kumar, Neuraldemy  \n**Course**: Natural Language Processing   \n**Notebook No**: 01   \n**Website**: https://neuraldemy.com/    \n___\n\nReaders are expected to have gone through the theory discussed in our free NLP tutorial. ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Basic Tokenization: \nThe simplest way to tokenize our corpus is by returning the output in this manner: ","metadata":{}},{"cell_type":"code","source":"text = \"\"\"The simplest way to tokenize our corpus is by returning the output in this manner\"\"\"\n\ndef basic_tokenizer(text):\n    tokens = text.split()\n    return tokens           ","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.470724Z","iopub.execute_input":"2024-05-07T14:16:36.471648Z","iopub.status.idle":"2024-05-07T14:16:36.477489Z","shell.execute_reply.started":"2024-05-07T14:16:36.471604Z","shell.execute_reply":"2024-05-07T14:16:36.475889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_tokenizer(text)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.495113Z","iopub.execute_input":"2024-05-07T14:16:36.495529Z","iopub.status.idle":"2024-05-07T14:16:36.504554Z","shell.execute_reply.started":"2024-05-07T14:16:36.495499Z","shell.execute_reply":"2024-05-07T14:16:36.503005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the problem with this approach is our tokenizer won't be able to deal with many things such as punctuation, special characters, or multi-word expressions. Let me show you some limitations:","metadata":{}},{"cell_type":"code","source":"text1 =  \"I love natural language processing, don't you?\"\nex1 = basic_tokenizer(text1)\nprint(\"Example 1:\", ex1)\n\ntext2 = \"The cat sat on the Mat.\"\ntext3 = \"the CAT SAT ON the mat.\"\nprint(\"Example 2:\", basic_tokenizer(text2), basic_tokenizer(text3))\n\ntext4 = \"It's raining cats and dogs.\"\nprint(\"Example 3:\", basic_tokenizer(text4))\n\ntext5 = \"Visit us at https://example.com or email us at info@example.com.\"\nprint(\"Example 4:\", basic_tokenizer(text5))\n\ntext6 = \"\"\"def basic_tokenizer(text):\n                tokens = text.split() \n                return tokens\"\"\"\nprint(\"Example 5:\", basic_tokenizer(text6))","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.534987Z","iopub.execute_input":"2024-05-07T14:16:36.535453Z","iopub.status.idle":"2024-05-07T14:16:36.545385Z","shell.execute_reply.started":"2024-05-07T14:16:36.535420Z","shell.execute_reply":"2024-05-07T14:16:36.543800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The problem in above examples are as explained below:\n1. The tokenizer splits the text only based on whitespace.This means \"processing,\" is not split into \"processing\" and \",\".\n2. The tokenizer does not handle case normalization, treating words with different cases as different tokens.  \"The\" and \"the\" are treated as separate tokens.\n3. In this example, in many cases you want to sepearate the terms It's as It and 's. \n4. URLs are being treated as single token which many not be suitable for many cases. We can use regex to identify the parts of those URLs.\n5. What if our corpus contains codes? We may not be able to take the code structure in account properly. \n\nSo, it seems like our basic tokenizer is not good enough. We need more sophiticated way to tokenize our text corpus. One thing I have mentioned several times in my tutorial, tokenization is task specific requirement and can be done in many ways.\n\nBefore we come to some advanced techniques to tokenize our corpus, let's see a tool that can help us analyse our text corpus. We will be using **NLTK library**. \n\n### NLTK\nNLTK is a natural language processing tooklit that allows us to do a lot of things. We will be using this tool for various tasks. Let's see some of it's methods on a text corpus. There are so many corpus available in NLTK that you can use in your practice projects. Check the link here: https://www.nltk.org/book/ch02.html","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import gutenberg\nfrom nltk import FreqDist\n\n# get the books ids and choose any one corpus for analysis\ngutenberg.fileids()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.547771Z","iopub.execute_input":"2024-05-07T14:16:36.548223Z","iopub.status.idle":"2024-05-07T14:16:36.566660Z","shell.execute_reply.started":"2024-05-07T14:16:36.548189Z","shell.execute_reply":"2024-05-07T14:16:36.565500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is the raw text from the book without any tokenization.\ntext = gutenberg.raw(\"austen-emma.txt\")\ntext[:400]","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.568768Z","iopub.execute_input":"2024-05-07T14:16:36.569210Z","iopub.status.idle":"2024-05-07T14:16:36.588320Z","shell.execute_reply.started":"2024-05-07T14:16:36.569152Z","shell.execute_reply":"2024-05-07T14:16:36.587047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NLTK offers various tokenizer that we can use to tokenize our corpus","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import WhitespaceTokenizer, word_tokenize, WordPunctTokenizer, RegexpTokenizer, TreebankWordTokenizer, sent_tokenize\n\n# Whitespace Tokenizer - Similar to what we have seen\nprint(\"-------- Whitespace Tokenizer --------\")\nwhitespace_tokenizer = WhitespaceTokenizer()\nwhitespace_tokens = whitespace_tokenizer.tokenize(text)\nprint(\"Tokens:\", whitespace_tokens[:10])\n\n# Word Tokenizer - This tokenizer splits the text into words based on punctuation and whitespace.\nprint(\"\\n-------- Word Tokenizer --------\")\nword_tokens = word_tokenize(text)\nprint(\"Tokens:\", word_tokens[:10])\n\n# WordPunct Tokenizer - Splits the text into words based on punctuation and whitespace, treating punctuation as separate tokens.\nprint(\"\\n-------- WordPunct Tokenizer --------\")\nwordpunct_tokenizer = WordPunctTokenizer()\nwordpunct_tokens = wordpunct_tokenizer.tokenize(text)\nprint(\"Tokens:\", wordpunct_tokens[:10])\n\n# Regexp Tokenizer - This tokenizer allows you to define a regular expression to specify how to split the text.\nprint(\"\\n-------- Regexp Tokenizer --------\")\nregexp_tokenizer = RegexpTokenizer(r'\\w+')\nregexp_tokens = regexp_tokenizer.tokenize(text)\nprint(\"Tokens:\", regexp_tokens[:10])\n\n# Sentence Tokenizer - Splits text into sentences using an algorithm described in Kiss and Strunk (2006)\nprint(\"\\n-------- Sentence Tokenizer --------\")\nsentences = sent_tokenize(text)\nprint(\"Sentences:\", sentences[:3])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:36.594454Z","iopub.execute_input":"2024-05-07T14:16:36.594842Z","iopub.status.idle":"2024-05-07T14:16:39.763192Z","shell.execute_reply.started":"2024-05-07T14:16:36.594809Z","shell.execute_reply":"2024-05-07T14:16:39.762055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's use word tokens to analyse some stats\n\n# Create a frequency distribution\nfdist = FreqDist(word_tokens)\n\n# Total number of samples\nprint(\"Total number of samples:\", fdist.N())\n\n# The 50 most common samples and their frequencies\nprint(\"The 50 most common samples and their frequencies:\")\nprint(fdist.most_common(50))\n\n# Sample with the greatest count\nprint(\"Sample with the greatest count:\", fdist.max())\n\n# Tabulate the frequency distribution\nprint(\"Frequency distribution:\")\nfdist.tabulate(10)\n\n# Graphical plot of the frequency distribution\nfdist.plot(30, cumulative=False)\n\n# Cumulative plot of the frequency distribution\nfdist.plot(30, cumulative=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:39.765062Z","iopub.execute_input":"2024-05-07T14:16:39.765426Z","iopub.status.idle":"2024-05-07T14:16:40.915871Z","shell.execute_reply.started":"2024-05-07T14:16:39.765398Z","shell.execute_reply":"2024-05-07T14:16:40.914545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see the most common words are basic english words but the most important one is \"Emma\". What about rare words? ","metadata":{}},{"cell_type":"code","source":"# Rare words - these words occur only once in the corpus. I have printed only first 50\nprint(\"Rare words:\", fdist.hapaxes()[:50])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:40.917259Z","iopub.execute_input":"2024-05-07T14:16:40.917703Z","iopub.status.idle":"2024-05-07T14:16:40.926908Z","shell.execute_reply.started":"2024-05-07T14:16:40.917663Z","shell.execute_reply":"2024-05-07T14:16:40.925474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vocabulary or unique words can be found using \nV = set(word_tokens)\nlen(V)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:40.930715Z","iopub.execute_input":"2024-05-07T14:16:40.931248Z","iopub.status.idle":"2024-05-07T14:16:40.950709Z","shell.execute_reply.started":"2024-05-07T14:16:40.931205Z","shell.execute_reply":"2024-05-07T14:16:40.949475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can also find ngrams etc using NLTK\nfrom nltk.util import ngrams\n\n# Unigrams\nunigrams = list(ngrams(word_tokens, 1))\n\n# Bigrams\nbigrams = list(ngrams(word_tokens, 2))\n\n# Trigrams\ntrigrams = list(ngrams(word_tokens, 3))\n\n# Print examples of unigrams, bigrams, and trigrams\nprint(\"Examples of Unigrams:\", unigrams[:5])\nprint(\"Examples of Bigrams:\", bigrams[:5])\nprint(\"Examples of Trigrams:\", trigrams[:5])","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:40.952155Z","iopub.execute_input":"2024-05-07T14:16:40.952533Z","iopub.status.idle":"2024-05-07T14:16:41.200631Z","shell.execute_reply.started":"2024-05-07T14:16:40.952503Z","shell.execute_reply":"2024-05-07T14:16:41.199559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can use this approach to create ngrams from your text corpus. NLTK really shines here! Or you can also create your own function:\n\n```def generate_ngrams(tokens, n):\n    ngrams_list = []\n    for i in range(len(tokens) - n + 1):\n        ngram = tuple(tokens[i:i + n])\n        ngrams_list.append(ngram)\n    return ngrams_list```","metadata":{}},{"cell_type":"markdown","source":"### Text Processing","metadata":{}},{"cell_type":"markdown","source":"Now, there are other ways to important our text corpus such as local files, blog posts, RSS feeds, web pages etc. To do that, we need to first find a way to get the information we are interested in, create a raw text format, and then convert it into tokens for further analysis.","metadata":{}},{"cell_type":"code","source":"# Processing Raw Text from the Web:\nfrom urllib import request\nurl = \"http://www.gutenberg.org/files/2554/2554-0.txt\" \n\nresponse = request.urlopen(url)\nraw = response.read().decode(\"utf8\")\nraw[:75]","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:41.201844Z","iopub.execute_input":"2024-05-07T14:16:41.202203Z","iopub.status.idle":"2024-05-07T14:16:41.704139Z","shell.execute_reply.started":"2024-05-07T14:16:41.202175Z","shell.execute_reply":"2024-05-07T14:16:41.702948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Another way to do the same thing is using requests lib\nimport requests\nresponse2 = requests.get(url)\nraw = response2.content.decode('utf8')\nraw[:75]","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:41.705729Z","iopub.execute_input":"2024-05-07T14:16:41.706131Z","iopub.status.idle":"2024-05-07T14:16:42.220558Z","shell.execute_reply.started":"2024-05-07T14:16:41.706092Z","shell.execute_reply":"2024-05-07T14:16:42.219190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with HTML in URLs\nfrom bs4 import BeautifulSoup\n\nurl = \"https://en.wikipedia.org/wiki/Main_Page\"\nresponse = requests.get(url)\nhtml = response.content\n\nsoup = BeautifulSoup(html, \"html.parser\")\ntext = soup.get_text()\ntext[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.222392Z","iopub.execute_input":"2024-05-07T14:16:42.222773Z","iopub.status.idle":"2024-05-07T14:16:42.690630Z","shell.execute_reply.started":"2024-05-07T14:16:42.222739Z","shell.execute_reply":"2024-05-07T14:16:42.689474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see using this HTML parse we have texts that contain so many page related items that we don't need we will have to get rid of them before we tokenize our texts. You can also use RSS feeds to get the texts in this manner. \n\n```\nimport feedparser\nllog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\nprint(\"Posts\", llog.entries)\npost = llog.entries[3]\nprint(\"Specific Post:\", post)\ncontent = post.content[0].value\ncontent[:100]```\n\n\nApart from this you can use local text files on your computer as well: \n\n``file_path = 'path/to/your/local/file.txt'\nwith open(file_path, 'r') as file:\n    text = file.read()``\n    \nAdditionally, you can create your own web crawler using Scrapy or use pre-crawled data available on https://commoncrawl.org/get-started. Please learn more about this website and build your program accordingly.\n\nIn addition to tokenization, I also discussed some normalization steps that are often involved before we tokenize our texts. These are Stemming And  Lemmatization","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import PorterStemmer\n\n# Initialize the PorterStemmer\nstemmer = PorterStemmer()\n\nsentence = \"Stemming reduces words to their root or base form. NLTK provides various stemmers, such as PorterStemmer and LancasterStemmer.\"\n# Tokenize the sentence\ntokens = nltk.word_tokenize(sentence)\n\n# Stemming\nstemmed_words = [stemmer.stem(word) for word in tokens]\nprint(\"Stemmed Words:\")\nprint(stemmed_words)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.692350Z","iopub.execute_input":"2024-05-07T14:16:42.693023Z","iopub.status.idle":"2024-05-07T14:16:42.702315Z","shell.execute_reply.started":"2024-05-07T14:16:42.692984Z","shell.execute_reply":"2024-05-07T14:16:42.701017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also use `lancaster = nltk.LancasterStemmer()`. For details on lemmatization, I recommend checking our this resource: https://www.nltk.org/book/ch03.html","metadata":{}},{"cell_type":"markdown","source":"### TF-IDF ","metadata":{}},{"cell_type":"markdown","source":"Sklearn also offers some tokenization options that we saw in theory. However for TF-IDF I would recommend using Gensim as discussed in the section after the next one. \n\n`CountVectorizer` converts a collection of text documents into a matrix of token counts.\n\nHow it works:\n* Tokenization: Splits the text into words (tokens).\n* Vocabulary Building: Builds a vocabulary of known words from the entire corpus.\n* Encoding: Encodes each document as a vector of word counts.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n\n# Let's say we have these documents\ndocuments = [\n    \"Feature extraction is very different from Feature selection\",\n    \"the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning\",\n    \"The latter is a machine learning technique applied on these features.\"\n]\n\n\n# CountVectorizer\nprint(\"CountVectorizer:\")\ncount_vectorizer = CountVectorizer()\ncount_matrix = count_vectorizer.fit_transform(documents)\nprint(count_matrix.toarray())\nprint(\"Vocabulary:\", count_vectorizer.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.706517Z","iopub.execute_input":"2024-05-07T14:16:42.707504Z","iopub.status.idle":"2024-05-07T14:16:42.730523Z","shell.execute_reply.started":"2024-05-07T14:16:42.707463Z","shell.execute_reply":"2024-05-07T14:16:42.729333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have already seen TF-IDF in the tutorial. This is how you can use it in Sklearn","metadata":{}},{"cell_type":"code","source":"print(\"\\nTfidfVectorizer:\")\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\nprint(tfidf_matrix.toarray())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.731772Z","iopub.execute_input":"2024-05-07T14:16:42.732135Z","iopub.status.idle":"2024-05-07T14:16:42.754133Z","shell.execute_reply.started":"2024-05-07T14:16:42.732100Z","shell.execute_reply":"2024-05-07T14:16:42.752918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can even write your own code using the described formula. Now let's see how we can calculate Pointwise Mutual Information (PMI). One way to do this using NLTK library: ","metadata":{}},{"cell_type":"markdown","source":"### Pointwise Mutual Information (PMI)","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\nfrom nltk.tokenize import word_tokenize\n\ntext = \"PMI Measure In NLTK library. Collocations are expressions of multiple words which commonly co-occur.\"\ntokens = word_tokenize(text)\n\n# finder object will be used to find significant bigrams in the text\nfinder = BigramCollocationFinder.from_words(tokens)\n\n# PMI score for each bigram and prints the results\nbigram_measures = BigramAssocMeasures()\nfinder.score_ngrams(bigram_measures.pmi)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.755851Z","iopub.execute_input":"2024-05-07T14:16:42.756795Z","iopub.status.idle":"2024-05-07T14:16:42.771386Z","shell.execute_reply.started":"2024-05-07T14:16:42.756755Z","shell.execute_reply":"2024-05-07T14:16:42.770131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Depending on the specific task at hand, we can select the most appropriate way to use the association scores of bigrams. For example, if we're interested in extracting significant phrases from text, we might filter bigrams based on a threshold PMI score. If we're building a text classification model, we might use bigrams as features along with other relevant features.","metadata":{}},{"cell_type":"code","source":"# n best score\nfinder.nbest(bigram_measures.pmi, 3)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T14:16:42.772927Z","iopub.execute_input":"2024-05-07T14:16:42.773303Z","iopub.status.idle":"2024-05-07T14:16:42.785435Z","shell.execute_reply.started":"2024-05-07T14:16:42.773272Z","shell.execute_reply":"2024-05-07T14:16:42.784149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I recommend going through this: https://www.nltk.org/howto/collocations.html.","metadata":{}},{"cell_type":"markdown","source":"### Vectors And Embeddings Using Gensim library\n\nGensim is a Python library for topic modeling, document similarity analysis, and word embeddings. It's designed to be efficient, scalable, and easy to use. Gensim provides implementations of several popular algorithms for natural language processing such as Word2Vec, Tf-IDF and Doc2Vec etc. \n\nThe core concepts of gensim are similar to what we have seen in tutorial: \n\n* Document: A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book.\n* Corpus: a collection of documents.\n* Vector: a mathematically convenient representation of a document.\n* Model: an algorithm for transforming vectors from one representation to another.\n* Bag-of-Words Model: Represent documents as vectors of word frequency counts, ignoring word order.\n\nIf you are serious about NLP or if your work involves many preprocessing steps then I highly recommend the Gensim library. Try to go through their documentation, they have so many interesting APIs that you can use. \n\nIn Gensim, the workflow typically involves these steps:\n\n- Corpus Creation: Start with a corpus of documents.\n- Vector Space Representation: Transform the documents into a vector space representation.\n- Model Creation: Create a model that transforms the original vector representation using algorithms such as TF-IDF, Word2Vec, or LDA.\n- Model Usage: Utilize the trained model for various tasks, such as document similarity, topic modeling, or document classification.\n\nNote: You don't have to learn everything. Just go through the library once and use according to the problem at hand.","metadata":{}},{"cell_type":"markdown","source":"**TF-IDF Model In Gensim**","metadata":{}},{"cell_type":"code","source":"from gensim import corpora\nfrom gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary\n\n# get your documents from local files or remote location\ndocuments = [\n    \"John likes to watch movies. Mary likes movies too.\",       # don't forget to add comma\n    \"John also likes to watch football games. Mary hates football.\"\n]\n\n# tokenize the documents or perform your normalization steps according to the tasks then tokenize\ntokenized_docs = [doc.lower().split() for doc in documents]\nprint(\"Tokenized Docs:\", tokenized_docs)\n\n# Then create dictionary\ndict = corpora.Dictionary(tokenized_docs)\nprint(\"Dictionary:\", dictionary)\n\n# Then create corpus\ncorpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\nprint(\"Corpus:\", corpus)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T16:23:07.631313Z","iopub.execute_input":"2024-05-07T16:23:07.631695Z","iopub.status.idle":"2024-05-07T16:23:07.638894Z","shell.execute_reply.started":"2024-05-07T16:23:07.631652Z","shell.execute_reply":"2024-05-07T16:23:07.638098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dictionary.token2id)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T16:23:10.700346Z","iopub.execute_input":"2024-05-07T16:23:10.701424Z","iopub.status.idle":"2024-05-07T16:23:10.706087Z","shell.execute_reply.started":"2024-05-07T16:23:10.701387Z","shell.execute_reply":"2024-05-07T16:23:10.704905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This show that how each word and their ids. Corpus shows the IDs and their counts. Notice that football and football. are two different tokens due punctuation mark.","metadata":{}},{"cell_type":"code","source":"# Then create TF-IDF model\ntfidf_model = TfidfModel(corpus)\nprint(tfidf_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T16:23:12.866061Z","iopub.execute_input":"2024-05-07T16:23:12.866651Z","iopub.status.idle":"2024-05-07T16:23:12.873385Z","shell.execute_reply.started":"2024-05-07T16:23:12.866610Z","shell.execute_reply":"2024-05-07T16:23:12.872009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-07T16:23:14.926071Z","iopub.execute_input":"2024-05-07T16:23:14.926461Z","iopub.status.idle":"2024-05-07T16:23:14.933623Z","shell.execute_reply.started":"2024-05-07T16:23:14.926431Z","shell.execute_reply":"2024-05-07T16:23:14.932462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conver the corpus from BOW vector to tf-IDF vector space\nvector = tfidf_model[corpus[0]]  # for the first document\nvector","metadata":{"execution":{"iopub.status.busy":"2024-05-07T16:23:17.165744Z","iopub.execute_input":"2024-05-07T16:23:17.166160Z","iopub.status.idle":"2024-05-07T16:23:17.172976Z","shell.execute_reply.started":"2024-05-07T16:23:17.166129Z","shell.execute_reply":"2024-05-07T16:23:17.172045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now this is your TF-IDF vector which you can use for further tasks. Notice I have only taken first document you can remove the index and find it for all. In practice use Gensim, not sklearn. ","metadata":{}},{"cell_type":"markdown","source":"**Word2Vec**\n\nWe already know about Word2Vec from the theory so no need to discuss it here. Let's see how we can use Gensim to create our  embeddings. \n\nThe Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual one-hot encoding of words goes through a ‘projection layer’ to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\n\nContinuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings. \n\n\nGensim allows you to train your own Word2Vec embeddings or use pre-trained embeddings. Read more about it here: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n\nThis is how you can train your Word2Vec model using Gensim:","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\n\n# Define a paragraph of text\nparagraph = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n\"\"\"\n\n# Preprocess the paragraph (tokenization is done internally by Word2Vec)\nsentences = [sentence.split() for sentence in paragraph.split('.')]  # Split into sentences\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:58:13.219236Z","iopub.execute_input":"2024-05-09T12:58:13.219703Z","iopub.status.idle":"2024-05-09T12:58:13.606429Z","shell.execute_reply.started":"2024-05-09T12:58:13.219671Z","shell.execute_reply":"2024-05-09T12:58:13.605396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel_path = \"word2vec_model\"\nmodel.save(model_path)\n\n# Load the model\nloaded_model = Word2Vec.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:58:25.448586Z","iopub.execute_input":"2024-05-09T12:58:25.449063Z","iopub.status.idle":"2024-05-09T12:58:25.461114Z","shell.execute_reply.started":"2024-05-09T12:58:25.449029Z","shell.execute_reply":"2024-05-09T12:58:25.459289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:58:32.191957Z","iopub.execute_input":"2024-05-09T12:58:32.192396Z","iopub.status.idle":"2024-05-09T12:58:32.200182Z","shell.execute_reply.started":"2024-05-09T12:58:32.192364Z","shell.execute_reply":"2024-05-09T12:58:32.198739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity = loaded_model.wv.similarity('language', 'processing')\nprint(f\"Similarity between 'language' and 'processing': {similarity:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:58:46.528405Z","iopub.execute_input":"2024-05-09T12:58:46.528908Z","iopub.status.idle":"2024-05-09T12:58:46.538299Z","shell.execute_reply.started":"2024-05-09T12:58:46.528872Z","shell.execute_reply":"2024-05-09T12:58:46.537012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find similar words\nsimilar_words = loaded_model.wv.most_similar(\"language\")\nprint(\"Most similar words to 'language':\", similar_words)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T12:58:59.833870Z","iopub.execute_input":"2024-05-09T12:58:59.834288Z","iopub.status.idle":"2024-05-09T12:58:59.844445Z","shell.execute_reply.started":"2024-05-09T12:58:59.834257Z","shell.execute_reply":"2024-05-09T12:58:59.842897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Must read their documentation API: https://radimrehurek.com/gensim/models/word2vec.html","metadata":{}},{"cell_type":"markdown","source":"#### Glove\nWhen it comes to Glove, Gensim offers a way to convert pre-trained Glove embeddings to word2vec but if you want to train your own Glove model, then I would recommend checking out this resource https://github.com/stanfordnlp/GloVe. For converting Glove embeddings to Word2Vec use this API mentioned here: https://radimrehurek.com/gensim/scripts/glove2word2vec.html","metadata":{}},{"cell_type":"markdown","source":"There are two more approaches to creating embeddings using Gensim: Doc2Vec(Works on sentence level rather than word) and FastText (Works on subword level). Do read them here: https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html and https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html","metadata":{}},{"cell_type":"markdown","source":"### Text Processing Using Keras/Tensorflow\nApart from the above mentioned libraries. Keras and tensorflow also offer their text-preprocessing (although deprecated) APIs that you can use: ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ncorpus = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"A brown fox is seen jumping over the sleeping dog.\",\n    \"The dog wakes up and chases the fox.\",\n    \"The fox escapes into the forest.\"\n]\n\n\n# tokenization \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:49:25.709755Z","iopub.execute_input":"2024-05-09T13:49:25.711559Z","iopub.status.idle":"2024-05-09T13:49:25.720763Z","shell.execute_reply.started":"2024-05-09T13:49:25.711486Z","shell.execute_reply":"2024-05-09T13:49:25.719188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:49:30.444504Z","iopub.execute_input":"2024-05-09T13:49:30.445002Z","iopub.status.idle":"2024-05-09T13:49:30.454367Z","shell.execute_reply.started":"2024-05-09T13:49:30.444967Z","shell.execute_reply":"2024-05-09T13:49:30.453269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padding: pad the sequences to ensure uniform length:\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nsequences = tokenizer.texts_to_sequences(corpus)\nprint(sequences)\npadded_sequences = pad_sequences(sequences)\npadded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:59:49.144960Z","iopub.execute_input":"2024-05-09T13:59:49.145422Z","iopub.status.idle":"2024-05-09T13:59:49.158669Z","shell.execute_reply.started":"2024-05-09T13:59:49.145391Z","shell.execute_reply":"2024-05-09T13:59:49.156627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from this, Keras also offeres the implementation various subsword algorithms that we will see next. ","metadata":{}},{"cell_type":"markdown","source":"### Byte Pair Encoding tokenizer\n\nNow, let's see how we can use BPE tokenizer which is what we will mostly use in practice in neural modelling. The next implementation is inspired by Andrej karpathy's [minbpe repository](https://github.com/karpathy/minbpe/tree/master). I highly recommend watching his video on Tokenization. Before we implement it, we need to understand things at low level. \n\nTexts are fundamentally encoded using some encoding and you have to know what encoding a particular text is using in order to create a right application that works well for everyone. Go through these basics first: \n\n- Bits and Memory: Computers store information using bits, which are like tiny on/off switches. Eight bits make a byte, the basic unit of memory.\n\n- ASCII: Early computers used ASCII (American Standard Code for Information Interchange). It assigned a unique 7-bit code to common characters like letters (a-z, A-Z), numbers (0-9), and basic symbols. This worked well for English, but not for other languages with special characters (¡¿☺).\n\n- Unicode: Unicode solved the issue with ASCII. It assigns a unique number (code point) to every character, regardless of language. This includes Latin characters, Cyrillic (used in Russian), Kanji (Japanese), and many more.  Unicode uses more than 7 bits, so it can handle all these characters.\n\n- Encoding: But how do we store these Unicode characters in our computer's memory (which still uses bytes)? This is where UTF-8 comes in. It's a way to translate (encode) Unicode characters into a sequence of bytes. With UTF-8, if a character can be represented with 1 byte that’s all it will use. If a character needs 4 bytes it’ll get 4 bytes (Don't get confuse with bits here). This is called a variable length encoding and it’s more efficient memory wise. Unicode encodings are simply how a piece of software implements the Unicode standard. Common characters from ASCII are stored the same way in UTF-8, most ASCII text files can be opened and read correctly with UTF-8 encoding. This makes it backward compatible. \n\n\nIn Python: \n\n- Python 3 internally stores text as Unicode strings. These strings use a variable number of bytes depending on the character. Each character has a unique code point, which is an integer representing its position in the Unicode standard. You don't need to worry about the specific encoding (like UTF-8) for most operations.\n\n- Working with Encodings (UTF-8): When you read text from a file, write text to a file, or communicate with external systems, you need to specify the encoding. Python defaults to UTF-8 encoding in most cases, which is the recommended choice for its efficiency and universality.\n\nTypes of representations:\n* **Binary (Base-2)**:\n   - Binary is the simplest numerical system, using only two symbols: 0 and 1.\n   - Each digit in a binary number represents a power of 2, starting from the rightmost digit.\n   - For example, the binary number `1010` represents  (1 * 2^3) + (0 * 2^2) + (1 * 2^1) + (0 * 2^0).\n\n* **Decimal (Base-10)**:\n   - Decimal is the number system we use every day, based on 10 digits from 0 to 9.\n   - Each digit in a decimal number represents a power of 10, starting from the rightmost digit.\n   - Same approach as previous (binary)\n\n* **Hexadecimal (Base-16)**:\n   - Hexadecimal uses 16 symbols: 0-9 and A-F (where A=10, B=11, ..., F=15).\n   - Each digit in a hexadecimal number represents a power of 16.\n\n* **Octal (Base-8)**:\n   - Octal uses 8 symbols: 0-7.\n   - Each digit in an octal number represents a power of 8.\n   \nIn memory, all numerical representations are ultimately stored as binary data.","metadata":{}},{"cell_type":"code","source":"# Decimal to other representations in Python\nnumber = 23     # decimal number\n\nbinary_string = bin(number)  \noctal_string = oct(number) \nhex_string = hex(number) \n\nprint(binary_string, octal_string, hex_string)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:56:19.236936Z","iopub.execute_input":"2024-05-09T17:56:19.237337Z","iopub.status.idle":"2024-05-09T17:56:19.242990Z","shell.execute_reply.started":"2024-05-09T17:56:19.237305Z","shell.execute_reply":"2024-05-09T17:56:19.241939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Other representations to decimal\nbinary_string = \"10111\"\noctal_string = \"027\"\nhex_string = \"17\"\n\ndecimal_from_binary = int(binary_string, 2)  \ndecimal_from_octal = int(octal_string, 8)  \ndecimal_from_hex = int(hex_string, 16) \n\nprint(decimal_from_binary, decimal_from_octal, decimal_from_hex)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:54:07.117591Z","iopub.execute_input":"2024-05-09T17:54:07.118103Z","iopub.status.idle":"2024-05-09T17:54:07.125889Z","shell.execute_reply.started":"2024-05-09T17:54:07.118062Z","shell.execute_reply":"2024-05-09T17:54:07.124796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this is clear. Now let's see unicode and UTF-8 example.","metadata":{}},{"cell_type":"code","source":"# Character 'A' in Unicode\nletter_a = 'A'\n\n# Access the code point (integer)\nprint(ord(letter_a))  # 65 (decimal representation)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:58:10.718256Z","iopub.execute_input":"2024-05-09T17:58:10.718651Z","iopub.status.idle":"2024-05-09T17:58:10.725254Z","shell.execute_reply.started":"2024-05-09T17:58:10.718621Z","shell.execute_reply":"2024-05-09T17:58:10.723646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Euro symbol in Unicode (needs more than 7 bits)\neuro_symbol = '€'\nprint(ord(euro_symbol))  # 8364 (decimal representation)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:58:25.306403Z","iopub.execute_input":"2024-05-09T17:58:25.307366Z","iopub.status.idle":"2024-05-09T17:58:25.312887Z","shell.execute_reply.started":"2024-05-09T17:58:25.307323Z","shell.execute_reply":"2024-05-09T17:58:25.311797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding a UTF-8 encoded byte string (assuming UTF-8 encoding)\nbyte_string = b'\\xe2\\x82\\xac'  # Euro symbol in UTF-8 bytes\ndecoded_symbol = byte_string.decode('utf-8')\nprint(decoded_symbol)  # € (Euro symbol)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:58:43.953220Z","iopub.execute_input":"2024-05-09T17:58:43.953594Z","iopub.status.idle":"2024-05-09T17:58:43.959294Z","shell.execute_reply.started":"2024-05-09T17:58:43.953565Z","shell.execute_reply":"2024-05-09T17:58:43.958161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding a Unicode string to UTF-8 bytes\nencoded_byte_string = euro_symbol.encode('utf-8')\nprint(encoded_byte_string)  # b'\\xe2\\x82\\xac' (Euro symbol in UTF-8 bytes)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T17:58:56.665408Z","iopub.execute_input":"2024-05-09T17:58:56.666163Z","iopub.status.idle":"2024-05-09T17:58:56.671184Z","shell.execute_reply.started":"2024-05-09T17:58:56.666126Z","shell.execute_reply":"2024-05-09T17:58:56.670159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is what goes behind the scene. If you are still confused about what are these encodings and why is this happening, I recommend reading these blog posts first then move ahead: \n\nMost important one: https://kunststube.net/encoding/ \n\nTwo similar posts: \n- https://deliciousbrains.com/how-unicode-works/\n- https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/","metadata":{}},{"cell_type":"markdown","source":"#### BPE Algorithm In Python","metadata":{}},{"cell_type":"code","source":"def token_ids(text):\n    \n    \"\"\"A function to convert the text corpus into raw \n    bytes then convert the values to integer IDs\"\"\"\n    \n    # Convert the text into raw bytes\n    tokens = text.encode(\"utf-8\")\n    \n    # Convert the values to list of integers in range 0..255 for convenience\n    tokens = list(map(int, tokens))\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:17:51.911413Z","iopub.execute_input":"2024-05-11T12:17:51.911780Z","iopub.status.idle":"2024-05-11T12:17:51.916998Z","shell.execute_reply.started":"2024-05-11T12:17:51.911749Z","shell.execute_reply":"2024-05-11T12:17:51.915999Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ids = token_ids(\"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄\")\nids","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:31.731301Z","iopub.execute_input":"2024-05-11T12:24:31.731679Z","iopub.status.idle":"2024-05-11T12:24:31.739401Z","shell.execute_reply.started":"2024-05-11T12:24:31.731649Z","shell.execute_reply":"2024-05-11T12:24:31.738325Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[239,\n 188,\n 181,\n 239,\n 189,\n 142,\n 239,\n 189,\n 137,\n 239,\n 189,\n 131,\n 239,\n 189,\n 143,\n 239,\n 189,\n 132,\n 239,\n 189,\n 133,\n 33,\n 32,\n 240,\n 159,\n 133,\n 164,\n 240,\n 159,\n 133,\n 157,\n 240,\n 159,\n 133,\n 152,\n 240,\n 159,\n 133,\n 146,\n 240,\n 159,\n 133,\n 158,\n 240,\n 159,\n 133,\n 147,\n 240,\n 159,\n 133,\n 148,\n 226,\n 128,\n 189,\n 32,\n 240,\n 159,\n 135,\n 186,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 179,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 174,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 168,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 180,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 169,\n 226,\n 128,\n 140,\n 240,\n 159,\n 135,\n 170,\n 33,\n 32,\n 240,\n 159,\n 152,\n 132]"},"metadata":{}}]},{"cell_type":"markdown","source":"Once we have these ids. We will calculate the pair counts:","metadata":{}},{"cell_type":"code","source":"def get_stats(ids, counts = None):\n    \"\"\" A function to get the pair counts from ids \"\"\"\n    counts = {} if counts is None else counts\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1  # if pair exist, we get the dict value and add one to count, else it returns 0\n    return counts","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:24:35.215496Z","iopub.execute_input":"2024-05-11T12:24:35.215859Z","iopub.status.idle":"2024-05-11T12:24:35.221676Z","shell.execute_reply.started":"2024-05-11T12:24:35.215831Z","shell.execute_reply":"2024-05-11T12:24:35.220749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"This function takes a list of items and calculates how often each pair of those items appears next to each other in the list. It does this by looping through neighboring items, checking if the pair has been seen before in a dictionary (or creating a new dictionary if none is provided), and then increasing the count for that pair. Finally, it returns the dictionary containing these counts, providing you with insights into how frequently specific item sequences occur in your data.","metadata":{}},{"cell_type":"code","source":"stats = get_stats(ids)\nlist(stats.items())[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:32:42.796285Z","iopub.execute_input":"2024-05-11T12:32:42.796613Z","iopub.status.idle":"2024-05-11T12:32:42.803803Z","shell.execute_reply.started":"2024-05-11T12:32:42.796588Z","shell.execute_reply":"2024-05-11T12:32:42.802763Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[((239, 188), 1),\n ((188, 181), 1),\n ((181, 239), 1),\n ((239, 189), 6),\n ((189, 142), 1)]"},"metadata":{}}]},{"cell_type":"markdown","source":"Now, it's time to merge them","metadata":{}},{"cell_type":"code","source":"def merge(ids, pair, idx):\n    \"\"\" A function to merge \"\"\"\n    newids = []\n    i = 0\n    while i < len(ids):\n        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n            newids.append(idx)\n            i += 2\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:44.690986Z","iopub.execute_input":"2024-05-11T12:48:44.691362Z","iopub.status.idle":"2024-05-11T12:48:44.698450Z","shell.execute_reply.started":"2024-05-11T12:48:44.691333Z","shell.execute_reply":"2024-05-11T12:48:44.697259Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"This function takes a list (ids), a pair of elements (pair), and an index (idx). It iterates through the ids list. If it finds the pair consecutively, it replaces that pair with the idx value in the merged list (newids). Otherwise, it keeps the elements from the original list (ids) in the merged list.","metadata":{}},{"cell_type":"code","source":"merge([4, 3, 2, 2, 4, 1], (2, 2), 44)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T12:48:57.021736Z","iopub.execute_input":"2024-05-11T12:48:57.022742Z","iopub.status.idle":"2024-05-11T12:48:57.028734Z","shell.execute_reply.started":"2024-05-11T12:48:57.022702Z","shell.execute_reply":"2024-05-11T12:48:57.027863Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[4, 3, 44, 4, 1]"},"metadata":{}}]},{"cell_type":"code","source":"text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:06:42.848597Z","iopub.execute_input":"2024-05-11T13:06:42.848949Z","iopub.status.idle":"2024-05-11T13:06:42.856397Z","shell.execute_reply.started":"2024-05-11T13:06:42.848923Z","shell.execute_reply":"2024-05-11T13:06:42.855095Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Let's perform merging for the above example\nvocab_size = 300 # the number of unique tokens you want to have\nnum_merges = vocab_size - 256 # It subtracts the reserved range (0-255) for basic ASCII characters \nids = list(token_ids(text)) # get tokens list as seen previously\n\nmerges = {}\nfor i in range(num_merges):\n    stats = get_stats(ids)     # get stats \n    pair = max(stats, key = stats.get) # finds the key (pair) in stats with the highest value (count)\n    idx = 256 + i # assigns a unique index to the new merged token\n    print(f\"merging {pair} into new token {idx}\")\n    ids = merge(ids, pair, idx)  # create new ids\n    merges[pair] = idx  # update the merges dict","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:06:56.411910Z","iopub.execute_input":"2024-05-11T13:06:56.413008Z","iopub.status.idle":"2024-05-11T13:06:56.523713Z","shell.execute_reply.started":"2024-05-11T13:06:56.412970Z","shell.execute_reply":"2024-05-11T13:06:56.521776Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"merging (101, 32) into new token 256\nmerging (115, 32) into new token 257\nmerging (105, 110) into new token 258\nmerging (116, 32) into new token 259\nmerging (116, 104) into new token 260\nmerging (101, 114) into new token 261\nmerging (226, 128) into new token 262\nmerging (99, 111) into new token 263\nmerging (32, 97) into new token 264\nmerging (97, 114) into new token 265\nmerging (111, 114) into new token 266\nmerging (100, 32) into new token 267\nmerging (44, 32) into new token 268\nmerging (111, 32) into new token 269\nmerging (263, 100) into new token 270\nmerging (258, 103) into new token 271\nmerging (101, 110) into new token 272\nmerging (105, 116) into new token 273\nmerging (111, 110) into new token 274\nmerging (46, 32) into new token 275\nmerging (97, 108) into new token 276\nmerging (97, 110) into new token 277\nmerging (116, 105) into new token 278\nmerging (116, 269) into new token 279\nmerging (32, 260) into new token 280\nmerging (101, 115) into new token 281\nmerging (262, 153) into new token 282\nmerging (270, 256) into new token 283\nmerging (111, 102) into new token 284\nmerging (111, 117) into new token 285\nmerging (85, 110) into new token 286\nmerging (286, 105) into new token 287\nmerging (121, 32) into new token 288\nmerging (108, 101) into new token 289\nmerging (271, 32) into new token 290\nmerging (287, 283) into new token 291\nmerging (115, 116) into new token 292\nmerging (97, 99) into new token 293\nmerging (264, 110) into new token 294\nmerging (284, 32) into new token 295\nmerging (240, 159) into new token 296\nmerging (97, 116) into new token 297\nmerging (108, 108) into new token 298\nmerging (114, 111) into new token 299\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"tokens length:\", len(token_ids(text)))\nprint(\"ids length:\", len(ids))\nprint(f\"compression ratio: {len(token_ids(text)) / len(ids):.2f}X\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:15:00.416663Z","iopub.execute_input":"2024-05-11T13:15:00.417008Z","iopub.status.idle":"2024-05-11T13:15:00.423870Z","shell.execute_reply.started":"2024-05-11T13:15:00.416980Z","shell.execute_reply":"2024-05-11T13:15:00.422553Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"tokens length: 4577\nids length: 3098\ncompression ratio: 1.48X\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Below are two helper functions to deal with control characters like \\n which are often present in our text corpus","metadata":{}},{"cell_type":"code","source":"import unicodedata\ndef replace_control_characters(s: str) -> str:\n    \n    \"\"\"This function, aims to remove or escape control characters \n    from a given string using incodedata\"\"\"\n    \n    chars = []\n    for ch in s:\n        if unicodedata.category(ch)[0] != \"C\":\n            chars.append(ch)\n        else:\n            chars.append(f\"\\\\u{ord(ch):04x}\")\n    return \"\".join(chars)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:00:48.538063Z","iopub.execute_input":"2024-05-11T14:00:48.538811Z","iopub.status.idle":"2024-05-11T14:00:48.549118Z","shell.execute_reply.started":"2024-05-11T14:00:48.538770Z","shell.execute_reply":"2024-05-11T14:00:48.547673Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"replace_control_characters(\"I am learning BPE and it's cool. \\n but it is also very messy\")","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:01:25.677788Z","iopub.execute_input":"2024-05-11T14:01:25.678805Z","iopub.status.idle":"2024-05-11T14:01:25.685245Z","shell.execute_reply.started":"2024-05-11T14:01:25.678751Z","shell.execute_reply":"2024-05-11T14:01:25.684247Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"\"I am learning BPE and it's cool. \\\\u000a but it is also very messy\""},"metadata":{}}]},{"cell_type":"code","source":"def render_token(t: bytes) -> str:\n    \n    \"\"\"This function, is designed to take a byte sequence (t) as input \n    and return a human-readable string (str) as output. It achieves this by \n    decoding the bytes and escaping any control characters present.\"\"\"\n    \n    s = t.decode(\"utf-8\", errors = \"replace\")  #\n    s = replace_control_characters(s)\n    return s","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:05:37.642508Z","iopub.execute_input":"2024-05-11T14:05:37.642867Z","iopub.status.idle":"2024-05-11T14:05:37.648121Z","shell.execute_reply.started":"2024-05-11T14:05:37.642839Z","shell.execute_reply":"2024-05-11T14:05:37.647240Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"t = b\"This string has \\na control character.\\n\"\nrender_token(t)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:09:08.632160Z","iopub.execute_input":"2024-05-11T14:09:08.632540Z","iopub.status.idle":"2024-05-11T14:09:08.639441Z","shell.execute_reply.started":"2024-05-11T14:09:08.632507Z","shell.execute_reply":"2024-05-11T14:09:08.638418Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"'This string has \\\\u000aa control character.\\\\u000a'"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's create a Tokenizer class","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    \n    def __init__(self):\n        \n        self.merges = {}\n        self.pattern = \"\"\n        self.special_tokens = {}\n        self.vocab = _build_vocab()\n        \n    def train(self, text, vocab_size, verbose = False):\n        raise NotImplementedError\n    \n    def encode(self, text):\n        raise NotImplementedError\n    \n    def decode(self, ids):\n        raise NotImplementedError\n    \n    def _build_vocab(self):\n        vocab = {idx: bytes([idx]) for idx in range(256)}\n        for (p0, p1), idx in self.merges.items():\n            vocab[idx] = vocab[p0] + vocab[p1]\n        for special, idx in self.special_tokens.items():\n            vocab[idx] = special.encode(\"utf-8\")\n        return vocab\n    \n    def save(self, file_prefix):\n        \n        model_file = file_prefix + \".model\"\n        with open(model_file, 'w') as f:\n            f.write(\"minbpe v1\\n\")\n            f.write(f\"{self.pattern}\\n\")\n            f.write(f\"{len(self.special_tokens)}\\n\")\n            for special, idx in self.special_tokens.items():\n                f.write(f\"{special} {idx}\\n\")\n            for idx1, idx2 in self.merges:\n                f.write(f\"{idx1} {idx2}\\n\")\n        \n        vocab_file = file_prefix + \".vocab\"\n        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n            for idx, token in self.vocab.items():\n                s = render_token(token)\n                if idx in inverted_merges:\n                    idx0, idx1 = inverted_merges[idx]\n                    s0 = render_token(self.vocab[idx0])\n                    s1 = render_token(self.vocab[idx1])\n                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n                    \n                else:\n                    f.write(f\"[{s}] {idx}\\n\")\n                    \n    def load(self, model_file):\n        assert model_file.endswith(\".model\")\n        merges = {}\n        special_tokens = {}\n        idx = 256\n        with open(model_file, 'r', encoding=\"utf-8\") as f:\n            version = f.readline().strip()\n            assert version == \"minbpe v1\"\n            self.pattern = f.readline().strip()\n            num_special = int(f.readline().strip())\n            for _ in range(num_special):\n                special, special_idx = f.readline().strip().split()\n                special_tokens[special] = int(special_idx)\n            \n            for line in f:\n                idx1, idx2 = map(int, line.split())\n                merges[(idx1, idx2)] = idx\n                idx += 1\n                \n            self.merges = merges\n            self.special_tokens = special_tokens\n            self.vocab = self._build_vocab()        ","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:45:59.803519Z","iopub.execute_input":"2024-05-11T14:45:59.804864Z","iopub.status.idle":"2024-05-11T14:45:59.820886Z","shell.execute_reply.started":"2024-05-11T14:45:59.804821Z","shell.execute_reply":"2024-05-11T14:45:59.820096Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}