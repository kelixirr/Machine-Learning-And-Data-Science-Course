{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kelixirr/backpropagation-simple-implementation-3-layers?scriptVersionId=145184586\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Let's build a basic neural nets with 3 layers. One input layer, one hidden layer and an ouput layer. This is an example of fully connected neural nets. Wer will first randomly assign weights and then use backpropagation technique to train our model so that our model can reduce the error and predict better. ","metadata":{}},{"cell_type":"code","source":"# let's assign random weights \n\n# weights of input layer conecting to hidden layer\nw1 = 0.11\nw2 = 0.21\nw3 = 0.12\nw4 = 0.08\n\n# weights of hidden layer conecting to output layer\nw5 = 0.14\nw6 = 0.15","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:11:35.50067Z","iopub.execute_input":"2023-10-04T06:11:35.501252Z","iopub.status.idle":"2023-10-04T06:11:35.507132Z","shell.execute_reply.started":"2023-10-04T06:11:35.501214Z","shell.execute_reply":"2023-10-04T06:11:35.506172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's create our dataset and output\n\n#inputs \ni1 = 2\ni2 = 3\n\n# ouput\nactual = 1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:19:30.849706Z","iopub.execute_input":"2023-10-04T06:19:30.850085Z","iopub.status.idle":"2023-10-04T06:19:30.855522Z","shell.execute_reply.started":"2023-10-04T06:19:30.850054Z","shell.execute_reply":"2023-10-04T06:19:30.854296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forward Pass\n\nWe will use the previous information to calculate the values of hidden layer and then output. ","metadata":{}},{"cell_type":"code","source":"# let's calculate h1 - hidden layer neuron - 1\nh1 = i1 * w1 + i2 * w2\nh1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:11:39.619024Z","iopub.execute_input":"2023-10-04T06:11:39.619592Z","iopub.status.idle":"2023-10-04T06:11:39.627681Z","shell.execute_reply.started":"2023-10-04T06:11:39.619558Z","shell.execute_reply":"2023-10-04T06:11:39.626249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's calculate h2 - hidden layer neuron - 2\nh2 = i1 * w3 + i2 * w4\nh2 ","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:12:10.68052Z","iopub.execute_input":"2023-10-04T06:12:10.6815Z","iopub.status.idle":"2023-10-04T06:12:10.688988Z","shell.execute_reply.started":"2023-10-04T06:12:10.681456Z","shell.execute_reply":"2023-10-04T06:12:10.687483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have the values of hideen layer neuron 1 and 2. We can now multiply them with their weights w5 and w6","metadata":{}},{"cell_type":"code","source":"# multiplying h1 & h2 with weights w5, w6 to get output \nprediction = h1 * w5 + h2 * w6\nprediction","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:15:03.523686Z","iopub.execute_input":"2023-10-04T06:15:03.524103Z","iopub.status.idle":"2023-10-04T06:15:03.532071Z","shell.execute_reply.started":"2023-10-04T06:15:03.524071Z","shell.execute_reply":"2023-10-04T06:15:03.530466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have the output value. Now let's compare it with actual value","metadata":{}},{"cell_type":"code","source":"# let's calculate the error between predicted and actual values. We will use this formula\nerror = ((prediction - actual)**2)/2\nerror","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:19:33.800144Z","iopub.execute_input":"2023-10-04T06:19:33.800613Z","iopub.status.idle":"2023-10-04T06:19:33.810042Z","shell.execute_reply.started":"2023-10-04T06:19:33.800577Z","shell.execute_reply":"2023-10-04T06:19:33.808029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Backpropagation","metadata":{}},{"cell_type":"markdown","source":"So, this is the error we are getting. Please note: for future derivates simplicity we have used the formula 1/2 * ((prediction-actual)**2)","metadata":{}},{"cell_type":"markdown","source":"Now we can notice from the above calculation that input or dataset will be same. The only thing we can control or vary is its weights. So, we will try to change weights using backpropagation to find optimum weights that can help us reduce the error because right now we have a huge gap.","metadata":{}},{"cell_type":"markdown","source":"We will use backpropagtion to update the previous weights using gradient descent. The gradients will be calculated using error with respect to weights. So, the new weight formula will be:\n\nWx = wx - lr(dError/dwx) \n\nwhere Wx in capital is new weight and wx is old weight and lr is learning rate.\n\nWe will use chain rule to calculate the derivates. We will get this on w6 after applying chain rule. We can they take derivate and get the output:\n\nStep 1: Take derivative of error with respect to weight of last layer:\n- dError_dw6 = (dError_dPrediction) * (dPrediction_dw6) \n\nStep 2: After taking derivative you will get this:\n- dError_dw6 = (prediction - actual) * h2\n- dError-dw6 = delta * h2  , where delta is the difference between prediction and actual valua","metadata":{}},{"cell_type":"code","source":"# backpropagation flow from end to start. So let's start with end\n# Once you have taken the derivates of error to weights, you can update w6 and w5\n\nlr = 0.05 # learning rate\ndelta = (0.191 - 1) \nnew_w6 = w6 - (lr * delta * h2)\nnew_w5 = w5 - (lr * delta * h1)\n\nnew_w6, new_w5","metadata":{"execution":{"iopub.status.busy":"2023-10-04T06:50:08.867135Z","iopub.execute_input":"2023-10-04T06:50:08.867547Z","iopub.status.idle":"2023-10-04T06:50:08.876874Z","shell.execute_reply.started":"2023-10-04T06:50:08.86752Z","shell.execute_reply":"2023-10-04T06:50:08.875357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similiarly if you calculate the derivates with respect to w1, w2, w3 and w4 then you wil have 3 chains: Error with respect to prediction, prediction with respect to h1 and h1 with respect to w1 and formula for the new weights in inputs layer will become:\n","metadata":{}},{"cell_type":"code","source":"# new weights for the first layer will be: \nnew_w4 = w4 - (lr * delta * i2 * w6)\nnew_w3 = w3 - (lr * delta * i1 * w6)\nnew_w2 = w2 - (lr * delta * i2 * w5)\nnew_w1 = w1 - (lr * delta * i1 * w5)\n\nnew_w1, new_w2, new_w3, new_w4","metadata":{"execution":{"iopub.status.busy":"2023-10-04T07:00:37.505616Z","iopub.execute_input":"2023-10-04T07:00:37.506039Z","iopub.status.idle":"2023-10-04T07:00:37.514128Z","shell.execute_reply.started":"2023-10-04T07:00:37.506007Z","shell.execute_reply":"2023-10-04T07:00:37.513306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now that we have new weights let's calculate the forward pass check for error\nnew_h1 = i1 * new_w1 + i2 * new_w2\nnew_h2 = i1 * new_w3 + i2 * new_w4\n\nnew_h1, new_h2","metadata":{"execution":{"iopub.status.busy":"2023-10-04T07:03:43.708333Z","iopub.execute_input":"2023-10-04T07:03:43.708691Z","iopub.status.idle":"2023-10-04T07:03:43.71587Z","shell.execute_reply.started":"2023-10-04T07:03:43.708664Z","shell.execute_reply":"2023-10-04T07:03:43.714731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_prediction = new_h1 * new_w5 + new_h2 * new_w6\nnew_prediction","metadata":{"execution":{"iopub.status.busy":"2023-10-04T07:03:49.280342Z","iopub.execute_input":"2023-10-04T07:03:49.280716Z","iopub.status.idle":"2023-10-04T07:03:49.287138Z","shell.execute_reply.started":"2023-10-04T07:03:49.280687Z","shell.execute_reply":"2023-10-04T07:03:49.286238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we have our new prediction after updating weights using backpropagation and calculating new output by using updated weights","metadata":{}},{"cell_type":"code","source":"new_error = new_prediction - actual\nnew_error","metadata":{"execution":{"iopub.status.busy":"2023-10-04T07:05:12.020954Z","iopub.execute_input":"2023-10-04T07:05:12.021376Z","iopub.status.idle":"2023-10-04T07:05:12.027953Z","shell.execute_reply.started":"2023-10-04T07:05:12.021345Z","shell.execute_reply":"2023-10-04T07:05:12.026709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can continue to go forward, calculate error and update weights using backpropagataion until we reach a level where our error reduces significantly. This was a simple implementation. I have removed activation functions and other complicated calculations such as regularization for simplicity. This should give an idea of forward and backward pass. For visualization of the graphs and see hand calculation please visit this link for the source of this implemenation: https://hmkcode.com/ai/backpropagation-step-by-step/","metadata":{}}]}