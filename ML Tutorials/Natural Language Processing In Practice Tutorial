{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing In Practice Tutorial By Neuraldemy\n\nThis tutorial is part of Neuraldemy's in depth tutorial on NLP. This notebook contains discussion regarding how we can solve various NLP tasks using the concepts we learned in the theory. \n___ \n\n**Author:** Amritesh Kumar, Neuraldemy  \n**Course:** Natural Language Processing  \n**Notebook No:** 02  \n**Website:** https://neuraldemy.com/  \n___\n\nReaders are expected to have gone through the theory discussed in our free NLP tutorial and Notebook No 01 [https://github.com/kelixirr/Neuraldemy/blob/main/ML%20Tutorials/tokenization-text-processing-in-nlp.ipynb] .","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:18.309039Z","iopub.execute_input":"2024-05-22T05:29:18.309418Z","iopub.status.idle":"2024-05-22T05:29:36.052422Z","shell.execute_reply.started":"2024-05-22T05:29:18.309389Z","shell.execute_reply":"2024-05-22T05:29:36.051379Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-22 05:29:21.037100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-22 05:29:21.037226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-22 05:29:21.217275: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.054247Z","iopub.execute_input":"2024-05-22T05:29:36.055132Z","iopub.status.idle":"2024-05-22T05:29:36.061070Z","shell.execute_reply.started":"2024-05-22T05:29:36.055097Z","shell.execute_reply":"2024-05-22T05:29:36.059804Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In theory, we saw the concept of RNNs, GRUs and LSTMs. Tensorflow offers simple APIs that you can use to implement these architectures. In practice we first take the data --> tokenize it --> convert it into embeddings --> use the embeddings in model. We have already seen various tokenization schemes. Now let's go further! \n\nWe can use pre-trained embeddings (good for small sample datasets) or train embeddings at the time of model training. Tensorflow also allows various text processing. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding\n\n# data\ntext = [\n    \"KerasNLP is a natural language processing library that supports users through their entire development cycle.\",\n    \"Our workflows are built from modular components that have state-of-the-art preset weights and architectures,\",\n    \"When used out-of-the-box and are easily customizable when more control is needed.\",\n]\n\n# tokenization using Tensorflow\ntokenizer = Tokenizer()\nprint(tokenizer)\ntokenizer.fit_on_texts(text)\nsequence = tokenizer.texts_to_sequences(text)\nprint(sequence)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.062994Z","iopub.execute_input":"2024-05-22T05:29:36.063499Z","iopub.status.idle":"2024-05-22T05:29:36.096305Z","shell.execute_reply.started":"2024-05-22T05:29:36.063458Z","shell.execute_reply":"2024-05-22T05:29:36.095040Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<keras.src.legacy.preprocessing.text.Tokenizer object at 0x7b9380d62380>\n[[8, 1, 9, 10, 11, 12, 13, 2, 14, 15, 16, 17, 18, 19, 20], [21, 22, 3, 23, 24, 25, 26, 2, 27, 28, 4, 5, 29, 30, 31, 6, 32], [7, 33, 34, 4, 5, 35, 6, 3, 36, 37, 7, 38, 39, 1, 40]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you have multiple sequences, you can pad them using pad_sequences: https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences","metadata":{}},{"cell_type":"code","source":"# pad the sequences to have the same length\npadded_sequences = pad_sequences(sequence, padding = \"post\")\npadded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.100913Z","iopub.execute_input":"2024-05-22T05:29:36.101632Z","iopub.status.idle":"2024-05-22T05:29:36.113465Z","shell.execute_reply.started":"2024-05-22T05:29:36.101583Z","shell.execute_reply":"2024-05-22T05:29:36.112269Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([[ 8,  1,  9, 10, 11, 12, 13,  2, 14, 15, 16, 17, 18, 19, 20,  0,\n         0],\n       [21, 22,  3, 23, 24, 25, 26,  2, 27, 28,  4,  5, 29, 30, 31,  6,\n        32],\n       [ 7, 33, 34,  4,  5, 35,  6,  3, 36, 37,  7, 38, 39,  1, 40,  0,\n         0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see zeros have been added after. Now we can use Embedding layer to learn embeddings for the specific task https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 # vocab_size \nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.115897Z","iopub.execute_input":"2024-05-22T05:29:36.117549Z","iopub.status.idle":"2024-05-22T05:29:36.127868Z","shell.execute_reply.started":"2024-05-22T05:29:36.117500Z","shell.execute_reply":"2024-05-22T05:29:36.126888Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"41"},"metadata":{}}]},{"cell_type":"code","source":"embedding_dims = 10  # Embedding dimensions we want to have ","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.129239Z","iopub.execute_input":"2024-05-22T05:29:36.130239Z","iopub.status.idle":"2024-05-22T05:29:36.139476Z","shell.execute_reply.started":"2024-05-22T05:29:36.130202Z","shell.execute_reply":"2024-05-22T05:29:36.137867Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\n\nmodel = Sequential([\n    Embedding(input_dim = vocab_size, output_dim = embedding_dims), # this takes max_words and embedding dimensions you want \n    Flatten(),\n    Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# model.build(input_shape=(None, padded_sequences.shape[1])) - Use to have some value in summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.141584Z","iopub.execute_input":"2024-05-22T05:29:36.142082Z","iopub.status.idle":"2024-05-22T05:29:36.227547Z","shell.execute_reply.started":"2024-05-22T05:29:36.142028Z","shell.execute_reply":"2024-05-22T05:29:36.226249Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the basic structure of our Embedding layer. You can first take a real data convert it into train and test set. Then compile and train the model. I will show you soon. If you have pre-trained embeddings like Glove or Word2vec then you can use them in Embedding layer's weights parameter. At the time of writing this tutorial, there website seem to have some problem but here is how you can use pre-trained embeddings as Embedding layer's weight","metadata":{}},{"cell_type":"markdown","source":"```embedding_index = {}\nwith open('glove.6B.100d.tx', encoding = \"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype = \"float32\")\n        embedding_index[word] = coefs\nprint(len(embedding_index))```\n\n```embedding_dim =  # Define Dimension of GloVe embeddings\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector```\n            \n```model = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dims, weights=[embedding_matrix], input_length=padded_sequences.shape[1], trainable=False),\n    Flatten(),\n    Dense(1, activation='sigmoid')\n])\n```","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:14:54.809462Z","iopub.execute_input":"2024-05-21T11:14:54.809855Z","iopub.status.idle":"2024-05-21T11:14:55.049114Z","shell.execute_reply.started":"2024-05-21T11:14:54.809826Z","shell.execute_reply":"2024-05-21T11:14:55.046495Z"}}},{"cell_type":"markdown","source":"### End to End Example \n\nIn this example, I will show you how you can train the model and embeddings at the same time or add your own pre-trained embeddings","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import sequence","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.228989Z","iopub.execute_input":"2024-05-22T05:29:36.229626Z","iopub.status.idle":"2024-05-22T05:29:36.235003Z","shell.execute_reply.started":"2024-05-22T05:29:36.229590Z","shell.execute_reply":"2024-05-22T05:29:36.233892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import requests\nimport zipfile\nimport os\n\nurl = 'http://mng.bz/0tIo'\nzip_path = 'imdb_data.zip'\nextract_path = 'imdb_data'\n\nresponse = requests.get(url)\nwith open(zip_path, 'wb') as f:\n    f.write(response.content)\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_path)\nos.remove(zip_path)\n\nprint(\"Download and extraction completed!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:36.236463Z","iopub.execute_input":"2024-05-22T05:29:36.237365Z","iopub.status.idle":"2024-05-22T05:29:56.142106Z","shell.execute_reply.started":"2024-05-22T05:29:36.237326Z","shell.execute_reply":"2024-05-22T05:29:56.140535Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Download and extraction completed!\n","output_type":"stream"}]},{"cell_type":"code","source":"imdb_dir = \"/kaggle/working/imdb_data/aclImdb\"\ntrain_dir = os.path.join(imdb_dir, \"train\")\n\nlabels = []\ntexts = []\n\nfor label_type in ['pos', 'neg']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:56.146599Z","iopub.execute_input":"2024-05-22T05:29:56.147021Z","iopub.status.idle":"2024-05-22T05:29:57.304166Z","shell.execute_reply.started":"2024-05-22T05:29:56.146989Z","shell.execute_reply":"2024-05-22T05:29:57.302860Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np\n\nmaxlen = 100  # review length\ntraining_samples = 200   \nvalidation_samples = 10000\nmax_words = 10000  # Top words number\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\n\nprint(\"Unique Tokens\", len(word_index))","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:29:57.306062Z","iopub.execute_input":"2024-05-22T05:29:57.306617Z","iopub.status.idle":"2024-05-22T05:30:10.433592Z","shell.execute_reply.started":"2024-05-22T05:29:57.306567Z","shell.execute_reply":"2024-05-22T05:30:10.432077Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Unique Tokens 88582\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pad_sequences(sequences, maxlen = maxlen)\nlabels = np.asarray(labels)\n\nprint(data.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.435116Z","iopub.execute_input":"2024-05-22T05:30:10.435450Z","iopub.status.idle":"2024-05-22T05:30:10.769506Z","shell.execute_reply.started":"2024-05-22T05:30:10.435424Z","shell.execute_reply":"2024-05-22T05:30:10.768242Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(25000, 100)\n(25000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"data[:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.771000Z","iopub.execute_input":"2024-05-22T05:30:10.771682Z","iopub.status.idle":"2024-05-22T05:30:10.781881Z","shell.execute_reply.started":"2024-05-22T05:30:10.771647Z","shell.execute_reply":"2024-05-22T05:30:10.780364Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,   11,    6,    3,  209,    4, 6702,   35, 2032,    3,\n        3789, 1226,    4, 3432,   18,    3,   29, 7093,    2, 3146, 7799,\n        2444,   29,    4,    3, 2103,   28,  558,   77, 9878,  126,  480,\n           2, 1439,  282,    7,    7,   11,   19,  395,    2,  523,    6,\n           3, 1525,   12, 3129,  946,   16,  304,  113,   31,   29, 1555,\n        2387,    6, 5975,   14,  641,    2, 7262,    6,    3, 4632,  400,\n           2, 5558,   12,    6,   21,    5,   27, 1045,    3,  368,  386,\n          19],\n       [9259,    6,    8,    1,    3,   62,    4,   32, 4416,   34,  457,\n        8595,   31,    1,  497,    2,    8,    1,  972, 2847, 2178,   24,\n         110,    2,    1, 1918,   60, 1072,    1,  129,   26,   44,  410,\n        2353,    8,   49,    2,  442,    8,    1, 4287,    4,   24,   24,\n         116,  599, 5074,    2, 1135, 7094, 2602, 5120,    2,   22,   25,\n           3,  450, 8596,   16, 3036,    2, 1975,  385,   16,    1, 1023,\n         931,    4, 2137,    2,    1, 3022,    4,  309, 4416,  294,   32,\n         318,   19,   15,  145,   80,  807, 3264,    1, 4416,  294, 5034,\n          15, 3023,    6,   32, 5514,    4,    1, 1299, 2205,  493,    1,\n          19]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"labels[:4] ","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.784041Z","iopub.execute_input":"2024-05-22T05:30:10.785138Z","iopub.status.idle":"2024-05-22T05:30:10.795002Z","shell.execute_reply.started":"2024-05-22T05:30:10.785093Z","shell.execute_reply":"2024-05-22T05:30:10.793724Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 1, 1])"},"metadata":{}}]},{"cell_type":"markdown","source":"Since our data is ordered, we will shuffle the data and then we will create training and validation set","metadata":{}},{"cell_type":"code","source":"data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.796037Z","iopub.execute_input":"2024-05-22T05:30:10.796392Z","iopub.status.idle":"2024-05-22T05:30:10.808214Z","shell.execute_reply.started":"2024-05-22T05:30:10.796361Z","shell.execute_reply":"2024-05-22T05:30:10.807288Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nprint(data[:3])\nprint(labels[:3])","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.809387Z","iopub.execute_input":"2024-05-22T05:30:10.809766Z","iopub.status.idle":"2024-05-22T05:30:10.834328Z","shell.execute_reply.started":"2024-05-22T05:30:10.809721Z","shell.execute_reply":"2024-05-22T05:30:10.832979Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[[   5    7    7  233   10  553   56   13    8    3 1749 5890 2506 1169\n     7    7   56   13  128 1650    1 8607 1744   12   56   44 4652   15\n   762    2   66  577  313   12   26   66 1244    9   15   38    7    7\n     1  684  559  270    8    3  389  510    8    2    1  585  572    8\n     7    7   47   57   13    3  789   62    8   81 3402 3666    4 5415\n    10  178    5  132   20 1507   10  374   11   14   10   13    8 1167\n    30    1   55    2    3 7709    4 1926   66 1829    1 2269    8    1\n  2506 1547]\n [ 837  531   97   25   74 3443 3925    2    1   62   59   21   25 3109\n    15    9    1  274   13   49   54 1025  130   18   10   13  128  314\n  1781   50   70   29    3  229   67   78    6 5237   12  124  238   35\n    26   67   76  142   80    1  203  509   12   90   87  801  363   71\n    45  126    3  334    4  708 8501   71  329    1  271   39  103    1\n  6056  307   16  743 7344    2 5294 1652   60    6   79   21   52   49\n    18   91   61    2  531    2 4237  231    4  126  110  821  302    4\n   467  631]\n [   5   94    1  133  303 2029    2  117    1  347    2  243   35    6\n   110  515   81    4  603 1801 4485 5849   36  175 3536  526  679 1584\n     2    1  133  215   35   73 1296   10  815    9   18   10  121    1\n  2528   14 2266  243   81   78 4485   12   93   33   78  132  180   37\n    12   21  313    6 2734    2    9  200   21   57   27   15  313    5\n    27 2734   23   81  636   35   73 1729    5  145    4   39  150  594\n     2    8   48   93   10   89  101   35 1103   40  272  147  137  103\n     1   17]]\n[0 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = data[:training_samples]\ny_train = labels[:training_samples]\n\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T05:30:10.835631Z","iopub.execute_input":"2024-05-22T05:30:10.836092Z","iopub.status.idle":"2024-05-22T05:30:10.842537Z","shell.execute_reply.started":"2024-05-22T05:30:10.836058Z","shell.execute_reply":"2024-05-22T05:30:10.841252Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\n\nembedding_dim = 100\n\nmodel = Sequential()\nmodel.add(keras.layers.Embedding(max_words, embedding_dim))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(32, activation = \"relu\"))\nmodel.add(keras.layers.Dense(1, activation = \"sigmoid\"))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:02:49.469591Z","iopub.execute_input":"2024-05-22T06:02:49.470037Z","iopub.status.idle":"2024-05-22T06:02:49.505819Z","shell.execute_reply.started":"2024-05-22T06:02:49.470001Z","shell.execute_reply":"2024-05-22T06:02:49.504479Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"model.compile(optimizer = \"adam\",\n              loss = 'binary_crossentropy',\n              metrics = [\"acc\"])\n\nhistory = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = (x_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:06:16.715597Z","iopub.execute_input":"2024-05-22T06:06:16.716010Z","iopub.status.idle":"2024-05-22T06:06:30.015712Z","shell.execute_reply.started":"2024-05-22T06:06:16.715979Z","shell.execute_reply":"2024-05-22T06:06:30.014319Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 261ms/step - acc: 0.4623 - loss: 0.6945 - val_acc: 0.4950 - val_loss: 0.6944\nEpoch 2/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - acc: 0.9804 - loss: 0.5627 - val_acc: 0.4919 - val_loss: 0.6964\nEpoch 3/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - acc: 1.0000 - loss: 0.4324 - val_acc: 0.4904 - val_loss: 0.7105\nEpoch 4/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - acc: 1.0000 - loss: 0.2799 - val_acc: 0.5007 - val_loss: 0.7291\nEpoch 5/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - acc: 1.0000 - loss: 0.1467 - val_acc: 0.5059 - val_loss: 0.7275\nEpoch 6/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - acc: 1.0000 - loss: 0.0627 - val_acc: 0.5107 - val_loss: 0.7253\nEpoch 7/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - acc: 1.0000 - loss: 0.0242 - val_acc: 0.5148 - val_loss: 0.7297\nEpoch 8/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - acc: 1.0000 - loss: 0.0120 - val_acc: 0.5180 - val_loss: 0.7365\nEpoch 9/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - acc: 1.0000 - loss: 0.0072 - val_acc: 0.5199 - val_loss: 0.7418\nEpoch 10/10\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step - acc: 1.0000 - loss: 0.0055 - val_acc: 0.5181 - val_loss: 0.7455\n","output_type":"stream"}]},{"cell_type":"code","source":"model.layers[0].get_weights()[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:13:25.100042Z","iopub.execute_input":"2024-05-22T06:13:25.100479Z","iopub.status.idle":"2024-05-22T06:13:25.117366Z","shell.execute_reply.started":"2024-05-22T06:13:25.100447Z","shell.execute_reply":"2024-05-22T06:13:25.115223Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([[-0.07825149, -0.02763392,  0.06120417, ..., -0.00359867,\n         0.01569862, -0.04776594],\n       [-0.10503739, -0.03460389, -0.01676889, ...,  0.00166418,\n         0.04804178,  0.0267777 ],\n       [-0.09219195, -0.00346945, -0.01222248, ..., -0.08399238,\n        -0.06701405, -0.0142562 ],\n       ...,\n       [ 0.01450591, -0.04940612,  0.03669062, ...,  0.00605027,\n         0.04926065,  0.00379891],\n       [ 0.04000608, -0.04564024,  0.00181786, ...,  0.02050677,\n         0.04169681, -0.04678595],\n       [ 0.01373564, -0.04488477,  0.03515809, ...,  0.04670436,\n        -0.0463688 ,  0.0315416 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"test_dir = os.path.join(imdb_dir, \"test\")\n\nlabels = []\ntexts = []\n\nfor label_type in ['pos', 'neg']:\n    dir_name = os.path.join(test_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:30:40.873919Z","iopub.execute_input":"2024-05-22T06:30:40.874376Z","iopub.status.idle":"2024-05-22T06:30:42.636715Z","shell.execute_reply.started":"2024-05-22T06:30:40.874343Z","shell.execute_reply":"2024-05-22T06:30:42.635504Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(texts)\nx_test = pad_sequences(sequences, maxlen=maxlen)\ny_test = np.asarray(labels)\nmodel.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:31:32.412486Z","iopub.execute_input":"2024-05-22T06:31:32.412919Z","iopub.status.idle":"2024-05-22T06:31:40.731288Z","shell.execute_reply.started":"2024-05-22T06:31:32.412887Z","shell.execute_reply":"2024-05-22T06:31:40.730261Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - acc: 0.5940 - loss: 0.6730\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[0.7473777532577515, 0.5148000121116638]"},"metadata":{}}]},{"cell_type":"markdown","source":"I am not able to access the Glove embedding website. You can try on your own and download it. \n`model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False` \nUse this to add the pre-trained weights and see how model performs.","metadata":{}},{"cell_type":"markdown","source":"Keras offers many APIs in recurrent layers: \n\n- LSTM layer\n- LSTM cell layer\n- GRU layer\n- GRU Cell layer\n- SimpleRNN layer\n- Bidirectional layer\n- Base RNN layer\n- Simple RNN cell layer\n- Stacked RNN cell layer\n\nThese are a few of them. If you notice there are cell layer and there are just Layer variants. Cell layers allow you to manage the cell level settings and layer allows you to manage the layer level settings. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}