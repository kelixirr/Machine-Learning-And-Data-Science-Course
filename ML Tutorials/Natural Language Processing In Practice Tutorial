{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing In Practice Tutorial By Neuraldemy\n\nThis tutorial is part of Neuraldemy's in depth tutorial on NLP. This notebook contains discussion regarding how we can solve various NLP tasks using the concepts we learned in the theory. \n___ \n\n**Author:** Amritesh Kumar, Neuraldemy  \n**Course:** Natural Language Processing  \n**Notebook No:** 02  \n**Website:** https://neuraldemy.com/  \n___\n\nReaders are expected to have gone through the theory discussed in our free NLP tutorial and Notebook No 01 [https://github.com/kelixirr/Neuraldemy/blob/main/ML%20Tutorials/tokenization-text-processing-in-nlp.ipynb] .","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:08:00.947650Z","iopub.execute_input":"2024-05-21T10:08:00.948790Z","iopub.status.idle":"2024-05-21T10:08:00.960431Z","shell.execute_reply.started":"2024-05-21T10:08:00.948744Z","shell.execute_reply":"2024-05-21T10:08:00.958986Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:08:00.939206Z","iopub.execute_input":"2024-05-21T10:08:00.939840Z","iopub.status.idle":"2024-05-21T10:08:00.946122Z","shell.execute_reply.started":"2024-05-21T10:08:00.939807Z","shell.execute_reply":"2024-05-21T10:08:00.944893Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In theory, we saw the concept of RNNs, GRUs and LSTMs. Tensorflow offers simple APIs that you can use to implement these architectures. In practice we first take the data --> tokenize it --> convert it into embeddings --> use the embeddings in model. We have already seen various tokenization schemes. Now let's go further! \n\nWe can use pre-trained embeddings (good for small sample datasets) or train embeddings at the time of model training. Tensorflow also allows various text processing. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding\n\n# data\ntext = [\n    \"KerasNLP is a natural language processing library that supports users through their entire development cycle.\",\n    \"Our workflows are built from modular components that have state-of-the-art preset weights and architectures,\",\n    \"When used out-of-the-box and are easily customizable when more control is needed.\",\n]\n\n# tokenization using Tensorflow\ntokenizer = Tokenizer()\nprint(tokenizer)\ntokenizer.fit_on_texts(text)\nsequence = tokenizer.texts_to_sequences(text)\nprint(sequence)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:42:48.519472Z","iopub.execute_input":"2024-05-21T10:42:48.519848Z","iopub.status.idle":"2024-05-21T10:42:48.528876Z","shell.execute_reply.started":"2024-05-21T10:42:48.519820Z","shell.execute_reply":"2024-05-21T10:42:48.527372Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<keras.src.legacy.preprocessing.text.Tokenizer object at 0x79d94751c880>\n[[8, 1, 9, 10, 11, 12, 13, 2, 14, 15, 16, 17, 18, 19, 20], [21, 22, 3, 23, 24, 25, 26, 2, 27, 28, 4, 5, 29, 30, 31, 6, 32], [7, 33, 34, 4, 5, 35, 6, 3, 36, 37, 7, 38, 39, 1, 40]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you have multiple sequences, you can pad them using pad_sequences: https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences","metadata":{}},{"cell_type":"code","source":"# pad the sequences to have the same length\npadded_sequences = pad_sequences(sequence, padding = \"post\")\npadded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:45:10.199119Z","iopub.execute_input":"2024-05-21T10:45:10.199539Z","iopub.status.idle":"2024-05-21T10:45:10.209188Z","shell.execute_reply.started":"2024-05-21T10:45:10.199509Z","shell.execute_reply":"2024-05-21T10:45:10.208016Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[ 8,  1,  9, 10, 11, 12, 13,  2, 14, 15, 16, 17, 18, 19, 20,  0,\n         0],\n       [21, 22,  3, 23, 24, 25, 26,  2, 27, 28,  4,  5, 29, 30, 31,  6,\n        32],\n       [ 7, 33, 34,  4,  5, 35,  6,  3, 36, 37,  7, 38, 39,  1, 40,  0,\n         0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see zeros have been added after. Now we can use Embedding layer to learn embeddings for the specific task https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 # vocab_size \nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:54:47.378969Z","iopub.execute_input":"2024-05-21T10:54:47.379369Z","iopub.status.idle":"2024-05-21T10:54:47.385750Z","shell.execute_reply.started":"2024-05-21T10:54:47.379338Z","shell.execute_reply":"2024-05-21T10:54:47.384421Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"41"},"metadata":{}}]},{"cell_type":"code","source":"embedding_dims = 10  # Embedding dimensions we want to have ","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:59:47.109234Z","iopub.execute_input":"2024-05-21T10:59:47.109640Z","iopub.status.idle":"2024-05-21T10:59:47.115162Z","shell.execute_reply.started":"2024-05-21T10:59:47.109613Z","shell.execute_reply":"2024-05-21T10:59:47.113832Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\n\nmodel = Sequential([\n    Embedding(input_dim = vocab_size, output_dim = embedding_dims), # this takes max_words and embedding dimensions you want \n    Flatten(),\n    Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# model.build(input_shape=(None, padded_sequences.shape[1])) - Use to have some value in summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:00:06.014668Z","iopub.execute_input":"2024-05-21T11:00:06.015297Z","iopub.status.idle":"2024-05-21T11:00:06.044158Z","shell.execute_reply.started":"2024-05-21T11:00:06.015256Z","shell.execute_reply":"2024-05-21T11:00:06.042873Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the basic structure of our Embedding layer. You can first take a real data convert it into train and test set. Then compile and train the model. I will show you soon. If you have pre-trained embeddings like Glove or Word2vec then you can use them in Embedding layer's weights parameter. At the time of writing this tutorial, there website seem to have some problem but here is how you can use pre-trained embeddings as Embedding layer's weight","metadata":{}},{"cell_type":"markdown","source":"```embedding_index = {}\nwith open('glove.6B.100d.tx', encoding = \"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype = \"float32\")\n        embedding_index[word] = coefs\nprint(len(embedding_index))```\n\n```embedding_dim =  # Define Dimension of GloVe embeddings\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector```\n            \n```model = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dims, weights=[embedding_matrix], input_length=padded_sequences.shape[1], trainable=False),\n    Flatten(),\n    Dense(1, activation='sigmoid')\n])\n```","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:14:54.809462Z","iopub.execute_input":"2024-05-21T11:14:54.809855Z","iopub.status.idle":"2024-05-21T11:14:55.049114Z","shell.execute_reply.started":"2024-05-21T11:14:54.809826Z","shell.execute_reply":"2024-05-21T11:14:55.046495Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keras offers many APIs in recurrent layers: \n\n- LSTM layer\n- LSTM cell layer\n- GRU layer\n- GRU Cell layer\n- SimpleRNN layer\n- Bidirectional layer\n- Base RNN layer\n- Simple RNN cell layer\n- Stacked RNN cell layer\n\nThese are a few of them. If you notice there are cell layer and there are just Layer variants. Cell layers allow you to manage the cell level settings and layer allows you to manage the layer level settings. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}