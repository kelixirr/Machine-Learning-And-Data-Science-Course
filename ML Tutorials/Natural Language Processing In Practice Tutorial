{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing In Practice Tutorial By Neuraldemy\n\nThis tutorial is part of Neuraldemy's in depth tutorial on NLP. This notebook contains discussion regarding how we can solve various NLP tasks using the concepts we learned in the theory. \n___ \n\n**Author:** Amritesh Kumar, Neuraldemy  \n**Course:** Natural Language Processing  \n**Notebook No:** 02  \n**Website:** https://neuraldemy.com/  \n___\n\nReaders are expected to have gone through the theory discussed in our free NLP tutorial and Notebook No 01 [https://github.com/kelixirr/Neuraldemy/blob/main/ML%20Tutorials/tokenization-text-processing-in-nlp.ipynb] .","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:08:00.947650Z","iopub.execute_input":"2024-05-21T10:08:00.948790Z","iopub.status.idle":"2024-05-21T10:08:00.960431Z","shell.execute_reply.started":"2024-05-21T10:08:00.948744Z","shell.execute_reply":"2024-05-21T10:08:00.958986Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pathlib\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:08:00.939206Z","iopub.execute_input":"2024-05-21T10:08:00.939840Z","iopub.status.idle":"2024-05-21T10:08:00.946122Z","shell.execute_reply.started":"2024-05-21T10:08:00.939807Z","shell.execute_reply":"2024-05-21T10:08:00.944893Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"In theory, we saw the concept of RNNs, GRUs and LSTMs. Tensorflow offers simple APIs that you can use to implement these architectures. In practice we first take the data --> tokenize it --> convert it into embeddings --> use the embeddings in model. We have already seen various tokenization schemes. Now let's go further! \n\nWe can use pre-trained embeddings (good for small sample datasets) or train embeddings at the time of model training. Tensorflow also allows various text processing. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding\n\n# data\ntext = [\n    \"KerasNLP is a natural language processing library that supports users through their entire development cycle.\",\n    \"Our workflows are built from modular components that have state-of-the-art preset weights and architectures,\",\n    \"When used out-of-the-box and are easily customizable when more control is needed.\",\n]\n\n# tokenization using Tensorflow\ntokenizer = Tokenizer()\nprint(tokenizer)\ntokenizer.fit_on_texts(text)\nsequence = tokenizer.texts_to_sequences(text)\nprint(sequence)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:42:48.519472Z","iopub.execute_input":"2024-05-21T10:42:48.519848Z","iopub.status.idle":"2024-05-21T10:42:48.528876Z","shell.execute_reply.started":"2024-05-21T10:42:48.519820Z","shell.execute_reply":"2024-05-21T10:42:48.527372Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<keras.src.legacy.preprocessing.text.Tokenizer object at 0x79d94751c880>\n[[8, 1, 9, 10, 11, 12, 13, 2, 14, 15, 16, 17, 18, 19, 20], [21, 22, 3, 23, 24, 25, 26, 2, 27, 28, 4, 5, 29, 30, 31, 6, 32], [7, 33, 34, 4, 5, 35, 6, 3, 36, 37, 7, 38, 39, 1, 40]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you have multiple sequences, you can pad them using pad_sequences: https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences","metadata":{}},{"cell_type":"code","source":"# pad the sequences to have the same length\npadded_sequences = pad_sequences(sequence, padding = \"post\")\npadded_sequences","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:45:10.199119Z","iopub.execute_input":"2024-05-21T10:45:10.199539Z","iopub.status.idle":"2024-05-21T10:45:10.209188Z","shell.execute_reply.started":"2024-05-21T10:45:10.199509Z","shell.execute_reply":"2024-05-21T10:45:10.208016Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"array([[ 8,  1,  9, 10, 11, 12, 13,  2, 14, 15, 16, 17, 18, 19, 20,  0,\n         0],\n       [21, 22,  3, 23, 24, 25, 26,  2, 27, 28,  4,  5, 29, 30, 31,  6,\n        32],\n       [ 7, 33, 34,  4,  5, 35,  6,  3, 36, 37,  7, 38, 39,  1, 40,  0,\n         0]], dtype=int32)"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see zeros have been added after. Now we can use Embedding layer to learn embeddings for the specific task https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding","metadata":{}},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 # vocab_size \nvocab_size","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:54:47.378969Z","iopub.execute_input":"2024-05-21T10:54:47.379369Z","iopub.status.idle":"2024-05-21T10:54:47.385750Z","shell.execute_reply.started":"2024-05-21T10:54:47.379338Z","shell.execute_reply":"2024-05-21T10:54:47.384421Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"41"},"metadata":{}}]},{"cell_type":"code","source":"embedding_dims = 10  # Embedding dimensions we want to have ","metadata":{"execution":{"iopub.status.busy":"2024-05-21T10:59:47.109234Z","iopub.execute_input":"2024-05-21T10:59:47.109640Z","iopub.status.idle":"2024-05-21T10:59:47.115162Z","shell.execute_reply.started":"2024-05-21T10:59:47.109613Z","shell.execute_reply":"2024-05-21T10:59:47.113832Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\n\nmodel = Sequential([\n    Embedding(input_dim = vocab_size, output_dim = embedding_dims), # this takes max_words and embedding dimensions you want \n    Flatten(),\n    Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# model.build(input_shape=(None, padded_sequences.shape[1])) - Use to have some value in summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:00:06.014668Z","iopub.execute_input":"2024-05-21T11:00:06.015297Z","iopub.status.idle":"2024-05-21T11:00:06.044158Z","shell.execute_reply.started":"2024-05-21T11:00:06.015256Z","shell.execute_reply":"2024-05-21T11:00:06.042873Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"This was the basic structure of our Embedding layer. You can first take a real data convert it into train and test set. Then compile and train the model. I will show you soon. If you have pre-trained embeddings like Glove or Word2vec then you can use them in Embedding layer's weights parameter. At the time of writing this tutorial, there website seem to have some problem but here is how you can use pre-trained embeddings as Embedding layer's weight","metadata":{}},{"cell_type":"markdown","source":"```embedding_index = {}\nwith open('glove.6B.100d.tx', encoding = \"utf-8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype = \"float32\")\n        embedding_index[word] = coefs\nprint(len(embedding_index))```\n\n```embedding_dim =  # Define Dimension of GloVe embeddings\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector```\n            \n```model = Sequential([\n    Embedding(input_dim=vocab_size, output_dim=embedding_dims, weights=[embedding_matrix], input_length=padded_sequences.shape[1], trainable=False),\n    Flatten(),\n    Dense(1, activation='sigmoid')\n])\n```","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:14:54.809462Z","iopub.execute_input":"2024-05-21T11:14:54.809855Z","iopub.status.idle":"2024-05-21T11:14:55.049114Z","shell.execute_reply.started":"2024-05-21T11:14:54.809826Z","shell.execute_reply":"2024-05-21T11:14:55.046495Z"}}},{"cell_type":"markdown","source":"### End to End Example Of How To Use Pre-Trained Embedding & Train Embeddings Using Embedding Layer","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing import sequence","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:48:15.139844Z","iopub.execute_input":"2024-05-21T15:48:15.140254Z","iopub.status.idle":"2024-05-21T15:48:15.145440Z","shell.execute_reply.started":"2024-05-21T15:48:15.140221Z","shell.execute_reply":"2024-05-21T15:48:15.144472Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import requests\nimport zipfile\nimport os\n\nurl = 'http://mng.bz/0tIo'\nzip_path = 'imdb_data.zip'\nextract_path = 'imdb_data'\n\nresponse = requests.get(url)\nwith open(zip_path, 'wb') as f:\n    f.write(response.content)\n\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_path)\nos.remove(zip_path)\n\nprint(\"Download and extraction completed!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:31:22.759375Z","iopub.execute_input":"2024-05-21T15:31:22.760167Z","iopub.status.idle":"2024-05-21T15:31:49.565437Z","shell.execute_reply.started":"2024-05-21T15:31:22.760133Z","shell.execute_reply":"2024-05-21T15:31:49.564272Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Download and extraction completed!\n","output_type":"stream"}]},{"cell_type":"code","source":"imdb_dir = \"/kaggle/working/imdb_data/aclImdb\"\ntrain_dir = os.path.join(imdb_dir, \"train\")\n\nlabels = []\ntexts = []\n\nfor label_type in ['pos', 'neg']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname))\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:40:44.524349Z","iopub.execute_input":"2024-05-21T15:40:44.524783Z","iopub.status.idle":"2024-05-21T15:40:45.533605Z","shell.execute_reply.started":"2024-05-21T15:40:44.524753Z","shell.execute_reply":"2024-05-21T15:40:45.532638Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport numpy as np\n\nmaxlen = 100  # review length\ntraining_samples = 200   \nvalidation_samples = 10000\nmax_words = 10000  # Top words number\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\n\nprint(\"Unique Tokens\", len(word_index))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:48:29.383743Z","iopub.execute_input":"2024-05-21T15:48:29.384140Z","iopub.status.idle":"2024-05-21T15:48:41.349777Z","shell.execute_reply.started":"2024-05-21T15:48:29.384109Z","shell.execute_reply":"2024-05-21T15:48:41.348653Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Unique Tokens 88582\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pad_sequences(sequences, maxlen = maxlen)\nlabels = np.asarray(labels)\n\nprint(data.shape)\nprint(labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:50:43.958717Z","iopub.execute_input":"2024-05-21T15:50:43.959143Z","iopub.status.idle":"2024-05-21T15:50:44.276512Z","shell.execute_reply.started":"2024-05-21T15:50:43.959110Z","shell.execute_reply":"2024-05-21T15:50:44.275056Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"(25000, 100)\n(25000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"data[:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:51:37.859287Z","iopub.execute_input":"2024-05-21T15:51:37.859702Z","iopub.status.idle":"2024-05-21T15:51:37.868359Z","shell.execute_reply.started":"2024-05-21T15:51:37.859672Z","shell.execute_reply":"2024-05-21T15:51:37.866973Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"array([[  69,   63,  406, 4174, 5794,   21,   61,   78,   33, 1079,  972,\n          30,   46,   52,  160,  208,    2,    8,   46,  160,  768, 7693,\n           1,  526,   12,   11,    6, 1213,    2,   18,   65, 1365,    6,\n         407,  157,    4,    1,   19,   33,   23, 1419,  530, 1419,  530,\n          10,  241, 1574,    4,    3,  225,  371,   16,   11,  169,  486,\n         472,    3,  225,  371,   41,    1,  228,    4,    3,  225,  371,\n        3249,   16,  254,  394,    2,   50, 6108,   10,  101,  200,   25,\n          66,  139,  726,    8,  327,  134,  472, 4174,  200,   21,   27,\n        4449, 3626,    4, 3366,    9,    6,  431, 1086, 1906, 2425,    2,\n        2888],\n       [ 489,   34,  453,    8,    3,  191,  679,  540,    2,   34,  263,\n           5, 1975,   87,    5,   11,  489,    1, 2032, 2052,    1,    2,\n          24,  593,  303,  676,    2,  613,   18,   11, 1439,  364,    1,\n          17,    7,    7,  148,    1,   62,  200,  478,  951,   39, 4892,\n          42,   21,   16,   63,   84,  351,  258,    4,    1,  333,    2,\n           1,  489,  803,   10,   89,  121,   65, 1429,    2,    3,   84,\n          62,    1,   17,   13, 1297,    2,  160,   30,    1,  169,   55,\n           7,    7,   45,   22,  185,    3,  576,    5,   64,   11,   17,\n          78,    9,   42,    3,   84, 5172,    5, 2471,  359,  434,    7,\n           7]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"labels[:4] ","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:52:39.738892Z","iopub.execute_input":"2024-05-21T15:52:39.739305Z","iopub.status.idle":"2024-05-21T15:52:39.748660Z","shell.execute_reply.started":"2024-05-21T15:52:39.739277Z","shell.execute_reply":"2024-05-21T15:52:39.747230Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 1, 1])"},"metadata":{}}]},{"cell_type":"markdown","source":"Since our data is ordered, we will shuffle the data and then we will create training and validation set","metadata":{}},{"cell_type":"code","source":"data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:55:00.349005Z","iopub.execute_input":"2024-05-21T15:55:00.349455Z","iopub.status.idle":"2024-05-21T15:55:00.356863Z","shell.execute_reply.started":"2024-05-21T15:55:00.349417Z","shell.execute_reply":"2024-05-21T15:55:00.355640Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"25000"},"metadata":{}}]},{"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nprint(data[:3])\nprint(labels[:3])","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:56:42.064275Z","iopub.execute_input":"2024-05-21T15:56:42.064724Z","iopub.status.idle":"2024-05-21T15:56:42.078119Z","shell.execute_reply.started":"2024-05-21T15:56:42.064691Z","shell.execute_reply":"2024-05-21T15:56:42.076933Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"[[ 156    7    7    3  305  369  295    3  247   34    6   21  924    8\n  1009  159 1641   94   53    2    1  797 2757  288  151  247  535    8\n    16   38  796  597    1 1959    4    7    7    3  144  212    9    6\n     5   64   86    3  182  305  247 3045   16    1 7838    4   65  843\n     2  656   38 1995  849   16   84  225   36 3376 2453 1145   14   70\n    14 1391  566  687    7    7 2206    5  103   18   45   22   14  543\n   123  253 4283    2  112   90    9    5    1  347   92   11   17   77\n    94   22]\n [ 120    6 6721    2  112 2855    4  171  546    8    3 1560  358 1237\n    44  185    5   39  139  588  384   95  137    4    1  943   45   10\n   130    3  975   36   12  120   10   77 1397   53    2    2 6979   58\n   521   43   16    3 6500  244   27  621   36    1 2115  306   71  103\n     3  365  317  531   12  415    4   10  128   78   21  388  135    1\n   120    6  128    8    1  943    2  617   72   29  121  930  177   12\n    72  178    5 1256  260  245 5500   51   72   64  390    8 1171   54\n    50  588]\n [ 187 2490 3390    6  421    5   27 4452  425 5370  739 1145 1088  269\n     2 9255   37   32 3047 7023 5019 1696    6    8  125 2993   26    2\n  2918    2 2669   26  269   37   32 2742   34   44 1410   24   43    5\n    27 1103   21  767   26  938    3  319    8    1 8522    1   84  116\n    62  269   37    3 1722   47    6  192 2598    8    1  664   18 3390\n     6 2368  182 8613  973    2 1219   51   26 4875    5   27   26   44\n     3  475 1384    2 1114  541    3   49  327    2    3 5310   12   26\n  1024    5]]\n[1 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = data[:training_samples]\ny_train = labels[:training_samples]\n\nx_val = data[training_samples: training_samples + validation_samples]\ny_val = labels[training_samples: training_samples + validation_samples]","metadata":{"execution":{"iopub.status.busy":"2024-05-21T15:58:38.529716Z","iopub.execute_input":"2024-05-21T15:58:38.530508Z","iopub.status.idle":"2024-05-21T15:58:38.535815Z","shell.execute_reply.started":"2024-05-21T15:58:38.530474Z","shell.execute_reply":"2024-05-21T15:58:38.534635Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# download the glove embeddings\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keras offers many APIs in recurrent layers: \n\n- LSTM layer\n- LSTM cell layer\n- GRU layer\n- GRU Cell layer\n- SimpleRNN layer\n- Bidirectional layer\n- Base RNN layer\n- Simple RNN cell layer\n- Stacked RNN cell layer\n\nThese are a few of them. If you notice there are cell layer and there are just Layer variants. Cell layers allow you to manage the cell level settings and layer allows you to manage the layer level settings. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}